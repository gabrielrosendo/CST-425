{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining â€“ Classification\n",
    "## Gabriel Marcelino, Eli Kaustinen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, write a comprehensive technical report as a markdown document, which includes all code, code comments, all outputs, plots, and analysis. Make sure the project documentation contains a) Problem statement, b) Algorithm of the solution, c) Analysis of the findings, and d) References.\n",
    "\n",
    "## Part I: Data Mining Techniques\n",
    "\n",
    "Explain each of the following data mining techniques in terms of how the algorithm works, its strengths, and weaknesses:\n",
    "\n",
    "### Classification: \n",
    "Classification algorithms categorize data into predefined labels or categories (like spam or not spam). They learn patterns from labeled training data and use that knowledge to classify new data. Common methods include decision trees, support vector machines (SVM), and neural networks. \n",
    "- Strengths: High accuracy with well-labeled data, useful for spam detection, medical diagnosis, and sentiment analysis.\n",
    "- Weaknesses: Performance drops with imbalanced or noisy data, and some models (e.g., deep learning) require significant computational resources.\n",
    "\n",
    "### Prediction\n",
    "Prediction models forecast future values based on historical data using regression techniques, time series analysis, or machine learning models.\n",
    "- Strengths: Useful for financial forecasting, sales predictions, and demand planning; can handle complex patterns.\n",
    "- Weaknesses: Accuracy depends on data quality and completeness, and it struggles with unpredictable external factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of each data mining functionality using a real-life data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 98.84%\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Features for last house: [ 3.55210000e+00  1.70000000e+01  3.98883929e+00  1.03348214e+00\n",
      "  1.67100000e+03  3.72991071e+00  3.42200000e+01 -1.18370000e+02]\n",
      "Prediction for last house(in $100,000s): [2.00940251]\n",
      "Actual price for last house (in $100,000s): 1.515\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Classification: Spam/Not Spam\n",
    "# read csv file emails.csv\n",
    "with open('emails.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "df = df.dropna()\n",
    "X = df['Message']\n",
    "y = df['Label']\n",
    "\n",
    "# Convert text to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X_transformed = vectorizer.fit_transform(X)\n",
    "X_transformed = X_transformed.toarray()\n",
    "\n",
    "# Separate features and labels for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Prediction: House Price Prediction\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "# Print column names\n",
    "print(data.feature_names)\n",
    "print(\"Features for last house:\", X_test[-1])\n",
    "print(\"Prediction for last house(in $100,000s):\", model.predict([X_test[-1]]))\n",
    "print(\"Actual price for last house (in $100,000s):\", y_test[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Access the \"UCI Machine Learning Repository,\" located in the topic Resources. Note: There are about 440 data sets that are suitable for use in a classification task. For this part of the exercise, you can choose one of these data sets, provided it includes at least 10 attributes and 10,000 instances. \n",
    "In class, we briefly discussed three classification methods: k-Nearest Neighbours (kNN), Support Vector Machine (SVM), and Decision Trees. For your selected data set, choose any two of the three classification methods and build a classifier based on each, as follows:\n",
    "\n",
    "Pre-process the data.\n",
    "Subset the data.\n",
    "Split the data into training and testing sets\n",
    "Build the classification model.\n",
    "Run the model (make predictions).\n",
    "Display classification results (quantitative and visual)\n",
    "Provide the confusion matrix for each classifier.\n",
    "For each classifier, compute the accuracy, sensitivity, and specificity.\n",
    "Explain the use of the ROC curve and the meaning of the area under the ROC curve.\n",
    "Compare the results obtained with each one of the classifiers, referring to the confusion matrix and associated metrics. Are the results similar? If not, how statistically different are they? If different, what is the reason? If similar, how would you decide to use one method or another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.geeksforgeeks.org/data-mining-techniques/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
